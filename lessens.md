git add .
git commit -m "   "
git push

# Le1
## CV历史
局部识别
整体识别
## 目标识别
 是否识别每一个物体
 数据集少+问题复杂导致高维模型
 ### 导致过拟合问题
## 课程内容

 图像分类问题
 图像摘要
 CNN 
# Le2 Classfication
 ## KNN
 分类模型的基本原理
 ### 比较函数
 1. L1 距离（差） ：（同坐标系相关）K最邻近算法
 * 向量每一方向的数值有着具体意义的时候用比较好。

 2. L2距离 ：同坐标系不相关

 ### 超参数
 * 例如使用L1 or L2
  不能从数据学习，要提前设定
  * 选择超参数时不能只根据训练模型的性能，还应该考虑鲁棒性即通用性。 

  * 验真超参好的办法：
  1. （对于大规模数据）将数据分为3组，大部分训练集，以及验证集和测试集，在训练集上用不同超参，用验证集评估，选择一组最后的超参不变，最后用测试集测出数据。（前面不能接触到测试集）
  2. 对于小规模数据，进行交叉验证。

 ## 线性分类
 f（x,W）=Wx+b
 * 用于计算同每一种类别之间的相似度
 * 线性分类器可能难以解决一些分类问题以及多分类问题。

# Le4

 梯度计算：

 解析法：利用微积分知识直接推导

 数值梯度：直接利用导数的定义去计算梯度（一般用来验证）

 ## 利用反向传播技术计算各个变量得到梯度

 1. 先画出损失函数的计算图

 2. 从后往前计算出每一个节点输出关于输入的梯度（本地梯度）

 3. 利用链式法则即可计算出。  

 * 当一个节点连接多个节点时，梯度在这个节点进行累加

 ## 神经网络

# L5
 
 ## CNN

 ### 全连接层

 之前学的W*x

 *  相当于前面的神经元同后面的神经元全部相关，所以相当于全连接层

 ### 卷积层

 * 相较于全连接层，它可以保全空间结构。

 * 全连接层将图片展开成向量，而卷积层并不需要这样做。

 * 一般会加入几圈0来保证输出和输入尺寸一样大

 #### 卷积核

 在图片上滑动，在每一个位置通过点积运算计算出一个值

 * 对于同一个卷积层，可以使用多个卷积核

 对于5*5的卷积核，这个5*5也可以叫做感受野

 ### pooling

 pooling操作主要是为了减少尺寸，为后续得出结果做铺垫

 * pooling操作不会改变深度，只会在平面上进行操作。

 通过这个网址来更好的理解CNN：http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html

# L6

 ## 数据预处理

 在图像领域，我们通常做0均值化，基本上不做归一化。（用CNN时）

 最终test阶段的数据处理应该要同预训练阶段的数据处理相同。

 W中权重过小会导致梯度变得非常小，变化的很慢，W权重过大会导致直接变成饱和。

 * 初始化权重非常重要！
 
 好的方法：Xavier初始化

 * 这个方法的本质是维持输入数据的方差和输出数据的方差相同。

 * 根据概率论知识，输出方差同输入到同一个神经元的数据的数目成正比，所以要维持输入方差和输出方差相同，W的方差要于输入到同一个神经元的数据的数目成反比（这里默认均值为0）

 * 对于ReLU函数需要将输入数据减半作为公式中“真实”的输入数据。

 批量归一化：目的是使梯度维持稳定，从而可以更好的训练

 ## 监视训练过程

 ### 选择合适的超参数

 进行交叉验证：用训练集训练，用验证集验证

 ### SGD可能出现的问题

 1. 关键原因在于这个方法只有一个全局的学习率，而通常在不同方向上的斜率是差别很大的，这样的话，过高的学习率会导致在高斜率的方向直接冲过头，导致优化速度变慢。而过低的学习率又会导致在斜率低的方向进步微乎其微。

 2. 无法正确处理局部最小点，或者鞍点 

 3. 由于是随机采样小批量，所以噪声问题也会存在 

 * 加入一个动量可以有效的解决这一些问题

 * 一般首选Adam方法

 会随着训练时间来降低学习率（一般和带动量的SGD结合的比较多，而和Adam结合的不多）

 * 原因可能是Adam它自身本来会随着训练变得越来越慢

 ### 出现过拟合问题怎么办

 1. 利用集成模型
 
 2. 考虑正则化 dropout非常常见（将部分激活函数置0）  

 ### 迁移学习

 用之前已经训练好的大模型的架构，然后自己根据具体需求修改最后一层或者对若干层微调

  